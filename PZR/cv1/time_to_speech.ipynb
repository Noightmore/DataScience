{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# CV1 - Time to Speech\n",
    "\n",
    "Jednoduchy program pro prevod casu (24hodinovy system) na hlasovou notifikaci.\n",
    "opus format byl zvolen kvuli jeho kvalite a nizke velikosti souboru. Opus je ztratova komprese, ktera je casto vyuzivana u audio streamovani.\n"
   ],
   "id": "573d5b388a5dccdf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import simpleaudio as sa\n",
    "from datetime import datetime\n",
    "import av\n",
    "from pathlib import Path\n",
    "from pydub.silence import detect_nonsilent\n",
    "from io import BytesIO\n",
    "import plotly.graph_objects as go\n",
    "from pydub import AudioSegment\n",
    "from pydub.effects import high_pass_filter, normalize, compress_dynamic_range\n",
    "import librosa\n",
    "import librosa.effects\n",
    "from datasets import load_dataset\n",
    "import shutil\n",
    "from huggingface_hub import snapshot_download"
   ],
   "id": "62d3a4b194e49f37",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Path to the folder with recordings\n",
    "RECORDINGS_DIR = \"./recordings\"\n",
    "RECORDINGS_DIR = Path(RECORDINGS_DIR)\n",
    "dataset_name = \"night12/czech_time_shards_recordings\""
   ],
   "id": "9b803c10f24ae04d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def download_recordings():\n",
    "    os.makedirs(RECORDINGS_DIR, exist_ok=True)\n",
    "\n",
    "    # Download all files from the dataset repository\n",
    "    downloaded_path = snapshot_download(dataset_name, local_dir=RECORDINGS_DIR, repo_type=\"dataset\")\n",
    "\n",
    "    print(f\"Dataset downloaded to: {downloaded_path}\")"
   ],
   "id": "3d8a604f50b65600",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "download_recordings()",
   "id": "9203da63b6930259",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_waveform(audio_segment, title=\"Waveform\"):\n",
    "    \"\"\" Visualize the waveform using Plotly \"\"\"\n",
    "    samples = np.array(audio_segment.get_array_of_samples(), dtype=np.int16)\n",
    "    time_axis = np.linspace(0, len(samples) / audio_segment.frame_rate, num=len(samples))\n",
    "\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=time_axis, y=samples, mode=\"lines\", name=\"Waveform\"))\n",
    "    fig.update_layout(title=title, xaxis_title=\"Time (seconds)\", yaxis_title=\"Amplitude\")\n",
    "    fig.show()\n"
   ],
   "id": "dadcd9e11ed34c9e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def play_opus(file_path):\n",
    "    try:\n",
    "        # Open Opus file with PyAV (FFmpeg backend)\n",
    "        container = av.open(file_path)\n",
    "        stream = next(s for s in container.streams if s.type == 'audio')\n",
    "\n",
    "        # Decode audio frames\n",
    "        audio_frames = []\n",
    "        sample_rate = stream.sample_rate\n",
    "        num_channels = stream.channels or 1  # Default to mono if not specified\n",
    "\n",
    "        for frame in container.decode(stream):\n",
    "            frame_data = frame.to_ndarray()\n",
    "\n",
    "            # Convert stereo to mono if necessary\n",
    "            if frame_data.ndim > 1:\n",
    "                frame_data = np.mean(frame_data, axis=0)  # Convert to mono by averaging channels\n",
    "\n",
    "            # Convert float32 data to int16 PCM (sound card expects this)\n",
    "            if frame_data.dtype == np.float32:\n",
    "                frame_data = (frame_data * 32767).astype(np.int16)\n",
    "\n",
    "            audio_frames.append(frame_data)\n",
    "\n",
    "        # Concatenate all frames into a single NumPy array\n",
    "        if len(audio_frames) == 0:\n",
    "            print(f\"❌ Error: No valid audio frames in {file_path}\")\n",
    "            return\n",
    "\n",
    "        audio_data = np.concatenate(audio_frames).astype(np.int16)\n",
    "\n",
    "        # Play audio using simpleaudio\n",
    "        play_obj = sa.play_buffer(audio_data, num_channels=num_channels, bytes_per_sample=2, sample_rate=sample_rate)\n",
    "        play_obj.wait_done()  # Wait for playback to finish\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error playing {file_path}: {e}\")"
   ],
   "id": "821a30f33b7247ab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "play_opus(\"recordings/minut.opus\")",
   "id": "f841ab9f1139488d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# POUHE PREHRAVANI AUDIO SOUBORU V SEKVENCI\n",
    "play_opus(\"recordings/je_prave.opus\")\n",
    "play_opus(\"recordings/20.opus\")\n",
    "play_opus(\"recordings/2.opus\")\n",
    "play_opus(\"recordings/hodin.opus\")\n",
    "play_opus(\"recordings/30.opus\")\n",
    "play_opus(\"recordings/4.opus\")\n",
    "play_opus(\"recordings/minut.opus\")"
   ],
   "id": "ef23d6483f71820f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def load_opus_with_pyav(file_path):\n",
    "    \"\"\" Load Opus file and return raw PCM data as AudioSegment \"\"\"\n",
    "    try:\n",
    "        container = av.open(file_path)\n",
    "        stream = next(s for s in container.streams if s.type == 'audio')\n",
    "\n",
    "        audio_frames = []\n",
    "        sample_rate = stream.sample_rate\n",
    "        num_channels = stream.channels or 1  # Default to mono if not specified\n",
    "\n",
    "        for frame in container.decode(stream):\n",
    "            frame_data = frame.to_ndarray()\n",
    "\n",
    "            # Debug: Print frame shape and dtype\n",
    "            #print(f\"Decoded frame shape: {frame_data.shape}, dtype: {frame_data.dtype}\")\n",
    "\n",
    "            # Convert stereo to mono if necessary\n",
    "            if frame_data.ndim > 1:\n",
    "                frame_data = np.mean(frame_data, axis=0)  # Convert to mono by averaging channels\n",
    "\n",
    "            # Convert float32 data to int16 PCM (sound card expects this)\n",
    "            if frame_data.dtype == np.float32:\n",
    "                frame_data = (frame_data * 32767).astype(np.int16)\n",
    "\n",
    "            audio_frames.append(frame_data)\n",
    "\n",
    "        if not audio_frames:\n",
    "            raise ValueError(f\"❌ No valid audio frames in {file_path}\")\n",
    "\n",
    "        # Concatenate all frames into a single NumPy array\n",
    "        audio_data = np.concatenate(audio_frames).astype(np.int16)\n",
    "\n",
    "        # Debug: Print sample statistics\n",
    "        #print(f\"Audio data stats - min: {audio_data.min()}, max: {audio_data.max()}, mean: {audio_data.mean()}\")\n",
    "\n",
    "        # Convert NumPy array to raw PCM byte stream\n",
    "        pcm_bytes = audio_data.tobytes()\n",
    "\n",
    "        # Wrap PCM bytes in BytesIO and create a pydub AudioSegment\n",
    "        audio_segment = AudioSegment(\n",
    "            data=pcm_bytes,\n",
    "            sample_width=2,  # 16-bit PCM\n",
    "            frame_rate=sample_rate,\n",
    "            channels=num_channels\n",
    "        )\n",
    "\n",
    "        return audio_segment\n",
    "\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"❌ Error decoding {file_path}: {e}\")\n"
   ],
   "id": "65368679e73e1225",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Function to flatten the nested list\n",
    "def flatten_list(nested_list):\n",
    "    flat_list = []\n",
    "    for item in nested_list:\n",
    "        if isinstance(item, list):\n",
    "            flat_list.extend(flatten_list(item))\n",
    "        else:\n",
    "            flat_list.append(item)\n",
    "    return flat_list"
   ],
   "id": "8664f3fbca51caca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def pick_correct_recordings_by_time(time_input, recordings_folder):\n",
    "    # split time into hours and minutes, forget seconds\n",
    "    hours = int(time_input.hour)\n",
    "    minutes = int(time_input.minute)\n",
    "\n",
    "    announcer = recordings_folder / \"je_prave.opus\"\n",
    "    hour_desc_file = None\n",
    "    minute_desc_file = None\n",
    "    hour_val = []\n",
    "    minute_val = []\n",
    "\n",
    "    if hours in (2,3,4):\n",
    "        announcer = recordings_folder / \"jsou_prave.opus\"\n",
    "\n",
    "    if hours == 1:\n",
    "        hour_desc_file = recordings_folder / f\"hodina.opus\"\n",
    "    elif 5 > hours > 1:\n",
    "        hour_desc_file = recordings_folder / f\"hodiny.opus\"\n",
    "    else:\n",
    "        hour_desc_file = recordings_folder / f\"hodin.opus\"\n",
    "\n",
    "    if hours == 2:\n",
    "        hour_val.append(recordings_folder / f\"dve.opus\")\n",
    "    elif hours < 20:\n",
    "        hour_val.append(recordings_folder / f\"{hours}.opus\")\n",
    "    else:\n",
    "        hour_val.append(recordings_folder / f\"{hours - hours % 10}.opus\")\n",
    "        if hours % 10 != 0:\n",
    "            hour_val.append(recordings_folder / f\"{hours % 10}.opus\")\n",
    "\n",
    "    if minutes == 1:\n",
    "        minute_desc_file = recordings_folder / f\"minuta.opus\"\n",
    "    elif minutes == 0:\n",
    "        minute_desc_file = recordings_folder / f\"silence.opus\"\n",
    "    elif 5 > minutes > 1:\n",
    "        minute_desc_file = recordings_folder / f\"minuty.opus\"\n",
    "    else:\n",
    "        minute_desc_file = recordings_folder / f\"minut.opus\"\n",
    "\n",
    "    if minutes == 0:\n",
    "        minute_val = []\n",
    "    elif minutes == 2:\n",
    "        minute_val.append(recordings_folder / f\"dve.opus\")\n",
    "    elif minutes < 20:\n",
    "        minute_val.append(recordings_folder / f\"{minutes}.opus\")\n",
    "    else:\n",
    "        if minutes % 10 == 0:\n",
    "            minute_val.append(recordings_folder / f\"{minutes}.opus\")\n",
    "        else:\n",
    "            minute_val.append(recordings_folder / f\"{minutes - minutes % 10}.opus\")\n",
    "            if minutes % 10 != 0:\n",
    "                minute_val.append(recordings_folder / f\"{minutes % 10}.opus\")\n",
    "\n",
    "\n",
    "    #print(hours, minutes)\n",
    "    #print(hour_val, hour_desc_file)\n",
    "    #print(minute_val, minute_desc_file)\n",
    "\n",
    "    return announcer, hour_val, hour_desc_file, minute_val, minute_desc_file\n",
    "    # returns a list of paths to the recordings"
   ],
   "id": "e20ef62e41900b23",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "pick_correct_recordings_by_time(datetime.now(), RECORDINGS_DIR)",
   "id": "6a1cf2ac863aa995",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def play_audio(audio_segment):\n",
    "    \"\"\" Play an AudioSegment object directly from RAM \"\"\"\n",
    "    samples = np.array(audio_segment.get_array_of_samples(), dtype=np.int16)\n",
    "\n",
    "    # Normalize PCM data if stereo\n",
    "    if audio_segment.channels == 2:\n",
    "        samples = samples.reshape((-1, 2))\n",
    "\n",
    "    # Debug: Check if samples contain valid audio\n",
    "    #print(f\"Playback samples min: {samples.min()}, max: {samples.max()}\")\n",
    "\n",
    "    # Play using simpleaudio\n",
    "    sa.play_buffer(samples, num_channels=audio_segment.channels, bytes_per_sample=2, sample_rate=audio_segment.frame_rate).wait_done()"
   ],
   "id": "213474708e25e31c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "audio = load_opus_with_pyav(\"recordings/je_prave.opus\")\n",
    "play_audio(audio)\n"
   ],
   "id": "c37472a1d3c1aa0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def assemble_speech_from_time(time_input, recordings_folder):\n",
    "    time_recording_files = pick_correct_recordings_by_time(time_input, recordings_folder)\n",
    "    time_recording_files = flatten_list(time_recording_files)\n",
    "    print(time_recording_files)\n",
    "    # now we need to read the binary data from the files and merge them into one sequence while trimming the silence spaces on the begging and end for each file\n",
    "\n",
    "    for file in time_recording_files:\n",
    "        audio = load_opus_with_pyav(file)\n",
    "        plot_waveform(audio, title=f\"Waveform of {file}\")\n",
    "        play_audio(audio)\n",
    "\n",
    "\n"
   ],
   "id": "6e8869b4eda268dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# vykresleni nahravek bez uprav\n",
    "assemble_speech_from_time(datetime.now(), RECORDINGS_DIR)"
   ],
   "id": "ef51d7cf4e66fcd2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# List of test times\n",
    "test_times = [\n",
    "    datetime(2024, 1, 1, 0, 0),  # 0:00\n",
    "    datetime(2024, 1, 1, 2, 0),  # 2:00\n",
    "    datetime(2024, 1, 1, 2, 2),  # 2:02\n",
    "    datetime(2024, 1, 1, 3, 10), # 3:10\n",
    "    datetime(2024, 1, 1, 4, 15), # 4:15\n",
    "    datetime(2024, 1, 1, 20, 19), # 20:19\n",
    "    datetime(2024, 1, 1, 20, 20) # 20:20\n",
    "]\n",
    "\n",
    "# Run the function with different datetime values\n",
    "for test_time in test_times:\n",
    "    print(f\"\\nTesting assemble_speech_from_time with time: {test_time.strftime('%H:%M')}\")\n",
    "    output_audio = assemble_speech_from_time(test_time, RECORDINGS_DIR)\n"
   ],
   "id": "471b3cfdbd8636a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Cast s rozsirenym resenim, obsahuje orez ticha, normalizaci hlasitosti, zmenu pitchu a tempa, zmenu hlasitosti",
   "id": "7dd57065e0d36105"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def boost_audio(audio_segment, factor=3):\n",
    "    \"\"\" Boost the audio volume by a given factor. \"\"\"\n",
    "\n",
    "    # Convert factor to decibels (logarithmic scale)\n",
    "    gain_db = 20 * np.log10(factor)\n",
    "\n",
    "    # Apply volume gain\n",
    "    boosted_audio = audio_segment.apply_gain(gain_db)\n",
    "\n",
    "    return boosted_audio\n"
   ],
   "id": "2fc5f82b1ca19a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def trim_silence(audio_segment, silence_thresh=-1, min_silence_len=50, keep_padding=50):\n",
    "    \"\"\" Trim leading and trailing silence where volume is lower than `silence_thresh`. \"\"\"\n",
    "\n",
    "    # Detect nonsilent regions (start and end of actual sound)\n",
    "    nonsilent_ranges = detect_nonsilent(audio_segment, min_silence_len=min_silence_len, silence_thresh=silence_thresh)\n",
    "\n",
    "    if not nonsilent_ranges:\n",
    "        print(\"⚠️ No speech detected, returning empty audio.\")\n",
    "        return AudioSegment.silent(duration=500)  # Return short silence to avoid errors\n",
    "\n",
    "    # Get start and end of non-silent part\n",
    "    start_trim = max(0, nonsilent_ranges[0][0] - keep_padding)  # Add buffer before start\n",
    "    end_trim = min(len(audio_segment), nonsilent_ranges[-1][1] + keep_padding)  # Add buffer after end\n",
    "\n",
    "    # Trim the audio\n",
    "    trimmed_audio = audio_segment[start_trim:end_trim]\n",
    "\n",
    "    return trimmed_audio\n"
   ],
   "id": "47ecf0a73e7e33a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def normalize_audio(audio_segment, target_dBFS=-20.0):\n",
    "    \"\"\"\n",
    "    Normalize the entire audio to a target loudness level.\n",
    "\n",
    "    Args:\n",
    "        audio_segment (AudioSegment): The input audio.\n",
    "        target_dBFS (float): Target loudness level in decibels (default: -20dBFS).\n",
    "\n",
    "    Returns:\n",
    "        AudioSegment: Normalized audio.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate current loudness\n",
    "    current_dBFS = audio_segment.dBFS\n",
    "    gain = target_dBFS - current_dBFS\n",
    "\n",
    "    # Apply gain to normalize\n",
    "    normalized_audio = audio_segment.apply_gain(gain)\n",
    "\n",
    "    return normalized_audio"
   ],
   "id": "3d1ce6650d95232e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def adjust_pitch(audio_segment, semitones=2):\n",
    "    \"\"\"\n",
    "    Adjust the pitch of an audio file without changing tempo.\n",
    "\n",
    "    Args:\n",
    "        audio_segment (AudioSegment): The input audio.\n",
    "        semitones (int): The number of semitones to shift.\n",
    "                         Positive = higher pitch, Negative = lower pitch.\n",
    "\n",
    "    Returns:\n",
    "        AudioSegment: The pitch-adjusted audio.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert AudioSegment to NumPy array\n",
    "    samples = np.array(audio_segment.get_array_of_samples(), dtype=np.float32)\n",
    "    sample_rate = audio_segment.frame_rate\n",
    "\n",
    "    # Ensure mono audio (librosa only processes mono)\n",
    "    if audio_segment.channels > 1:\n",
    "        samples = samples.reshape((-1, audio_segment.channels)).mean(axis=1)\n",
    "\n",
    "    # Apply pitch shifting\n",
    "    pitch_shifted_samples = librosa.effects.pitch_shift(samples, sr=sample_rate, n_steps=semitones)\n",
    "\n",
    "    # Convert back to AudioSegment\n",
    "    processed_audio = AudioSegment(\n",
    "        pitch_shifted_samples.astype(np.int16).tobytes(),\n",
    "        frame_rate=sample_rate,\n",
    "        sample_width=2,\n",
    "        channels=1\n",
    "    )\n",
    "\n",
    "    return processed_audio\n"
   ],
   "id": "a60fcbea62680cbc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## nasledujici metody byly pouzity pro pokus o odstraneni plosivnich zvuku\n",
    "nizka uspesnost, explosivni t je stale slyset"
   ],
   "id": "268d66f27ffbab0e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# meni i barvu zvuku, ale plosivni zvuky stale slyset temer stejne\n",
    "def remove_plosives(audio_segment, highpass_freq=22000, target_freq=5500, width=800, compression_threshold=-25):\n",
    "    \"\"\"\n",
    "    Reduce harsh plosive sounds using:\n",
    "    1. A High-Pass Filter to remove low-end pops.\n",
    "    2. A Notch Filter (FFT) to target the \"T\" frequency (5kHz-6kHz).\n",
    "    3. Hard Limiting to suppress explosive transients.\n",
    "\n",
    "    Args:\n",
    "        audio_segment (AudioSegment): The input audio.\n",
    "        highpass_freq (int): High-Pass filter cutoff (default: 2000 Hz).\n",
    "        target_freq (int): Frequency to notch out (default: 5500 Hz).\n",
    "        width (int): Range of frequencies to reduce (default: 800 Hz).\n",
    "        compression_threshold (int): Compression threshold in dB.\n",
    "\n",
    "    Returns:\n",
    "        AudioSegment: Processed audio with reduced plosives.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Apply High-Pass Filter to Remove Low-End Plosives\n",
    "    filtered_audio = high_pass_filter(audio_segment, highpass_freq)\n",
    "\n",
    "    # Step 2: Apply a De-Essing Notch Filter at 5.5kHz-6.3kHz (Reduces \"T\" Harshness)\n",
    "    def notch_filter(audio, freq, width):\n",
    "        samples = np.array(audio.get_array_of_samples(), dtype=np.float32)\n",
    "        fft = np.fft.rfft(samples)\n",
    "        freqs = np.fft.rfftfreq(len(samples), d=1/audio.frame_rate)\n",
    "\n",
    "        # Find & Reduce the Target Frequency Band\n",
    "        mask = (freqs > freq - width / 2) & (freqs < freq + width / 2)\n",
    "        fft[mask] *= 0.5  # Reduce \"T\" frequencies by 50%\n",
    "\n",
    "        # Inverse FFT to Get the Processed Audio Back\n",
    "        filtered_samples = np.fft.irfft(fft).astype(np.int16)\n",
    "        return AudioSegment(filtered_samples.tobytes(), frame_rate=audio.frame_rate, sample_width=2, channels=1)\n",
    "\n",
    "    reduced_t_audio = notch_filter(filtered_audio, target_freq, width)\n",
    "\n",
    "    # Step 3: Apply Hard Limiting to Catch Sudden Plosives\n",
    "    compressed_audio = compress_dynamic_range(reduced_t_audio, threshold=compression_threshold)\n",
    "\n",
    "    return compressed_audio\n"
   ],
   "id": "e73a6c4ae979c45",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# metoda se spatnymi vysledky pri redukci plosivnich zvuku\n",
    "def detect_and_reduce_plosive(audio_segment, spike_threshold=1000, reduction_factor=0.5):\n",
    "    \"\"\"\n",
    "    Detects and reduces plosive spikes in an audio file by:\n",
    "    1. Identifying sudden amplitude spikes.\n",
    "    2. Attenuating the plosive while keeping speech clarity.\n",
    "\n",
    "    Args:\n",
    "        audio_segment (AudioSegment): The input audio.\n",
    "        spike_threshold (int): The amplitude level to consider a spike.\n",
    "        reduction_factor (float): How much to reduce the spike (0.5 = 50% volume reduction).\n",
    "\n",
    "    Returns:\n",
    "        AudioSegment: The processed audio with plosives reduced.\n",
    "    \"\"\"\n",
    "\n",
    "    samples = np.array(audio_segment.get_array_of_samples(), dtype=np.int16)\n",
    "\n",
    "    # Find plosive peaks\n",
    "    spikes = np.where(np.abs(samples) > spike_threshold)[0]\n",
    "\n",
    "    if len(spikes) > 0:\n",
    "        print(f\"⚠️ Found {len(spikes)} plosive spikes, reducing...\")\n",
    "\n",
    "        # Reduce the amplitude of spikes\n",
    "        for idx in spikes:\n",
    "            samples[idx] = int(samples[idx] * reduction_factor)  # Reduce spike volume\n",
    "\n",
    "    # Convert back to AudioSegment\n",
    "    processed_audio = AudioSegment(\n",
    "        samples.tobytes(),\n",
    "        frame_rate=audio_segment.frame_rate,\n",
    "        sample_width=2,\n",
    "        channels=1\n",
    "    )\n",
    "\n",
    "    return processed_audio"
   ],
   "id": "e37ede4966e0ec8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# o neco lepsi metoda\n",
    "def adaptive_plosive_reduction(audio_segment, spike_threshold=22000, attack_window=10, release_window=30, reduction_factor=0.6):\n",
    "    \"\"\"\n",
    "    Reduce plosives without affecting normal speech:\n",
    "    1. Detects rapid amplitude bursts.\n",
    "    2. Selectively reduces only plosive parts.\n",
    "    3. Uses an adaptive attack & release to avoid muffling normal sounds.\n",
    "\n",
    "    Args:\n",
    "        audio_segment (AudioSegment): The input audio.\n",
    "        spike_threshold (int): The level above which a plosive is detected.\n",
    "        attack_window (int): How quickly suppression starts (samples).\n",
    "        release_window (int): How smoothly suppression fades out (samples).\n",
    "        reduction_factor (float): Strength of suppression (0.6 = 40% volume reduction).\n",
    "\n",
    "    Returns:\n",
    "        AudioSegment: Processed audio with reduced plosives.\n",
    "    \"\"\"\n",
    "\n",
    "    samples = np.array(audio_segment.get_array_of_samples(), dtype=np.int16)\n",
    "    sample_rate = audio_segment.frame_rate\n",
    "\n",
    "    # Find rapid spikes (potential plosives)\n",
    "    spike_indices = np.where(np.abs(samples) > spike_threshold)[0]\n",
    "\n",
    "    if len(spike_indices) > 0:\n",
    "        print(f\"⚠️ Detected {len(spike_indices)} plosive spikes, selectively reducing...\")\n",
    "\n",
    "        # Process each detected plosive\n",
    "        for idx in spike_indices:\n",
    "            start = max(0, idx - attack_window)\n",
    "            end = min(len(samples), idx + release_window)\n",
    "\n",
    "            # Apply a gradual reduction instead of a hard cut\n",
    "            fade_curve = np.linspace(1, reduction_factor, num=end-start)\n",
    "            samples[start:end] = (samples[start:end] * fade_curve).astype(np.int16)\n",
    "\n",
    "    # Convert back to AudioSegment\n",
    "    processed_audio = AudioSegment(\n",
    "        samples.tobytes(),\n",
    "        frame_rate=sample_rate,\n",
    "        sample_width=2,\n",
    "        channels=1\n",
    "    )\n",
    "\n",
    "    return processed_audio"
   ],
   "id": "d4c8e7e0c3db5e08",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def enhanced_assemble_speech_from_time(time_input, recordings_folder):\n",
    "    time_recording_files = pick_correct_recordings_by_time(time_input, recordings_folder)\n",
    "    time_recording_files = flatten_list(time_recording_files)\n",
    "    print(time_recording_files)\n",
    "\n",
    "\n",
    "    for file in time_recording_files:\n",
    "        audio = load_opus_with_pyav(file)\n",
    "        audio = trim_silence(audio, silence_thresh=-60, min_silence_len=10, keep_padding=50)\n",
    "        audio = normalize_audio(audio)\n",
    "        audio = adaptive_plosive_reduction(audio) # neni 100 % uspesne; pouzita nejnormalnejsi metoda\n",
    "        audio = boost_audio(audio, factor=5)\n",
    "        # audio = adjust_pitch(audio, semitones=5) # zvyseni pitchu o 5 polotonu; divny zvuk s plno explozema\n",
    "        play_audio(audio)\n",
    "\n",
    "\n"
   ],
   "id": "103fc34f8687fff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "enhanced_assemble_speech_from_time(datetime.now(), RECORDINGS_DIR)",
   "id": "37ab74eaf6eb2232",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# debugging exlosivnich T\n",
    "time_recording_files = pick_correct_recordings_by_time(datetime.now(), RECORDINGS_DIR)\n",
    "\n",
    "#audio = load_opus_with_pyav(time_recording_files[-1])\n",
    "audio = load_opus_with_pyav(\"recordings/minut.opus\")\n",
    "audio = trim_silence(audio, silence_thresh=-60, min_silence_len=10, keep_padding=50)\n",
    "plot_waveform(audio)\n",
    "\n",
    "audio = normalize_audio(audio)\n",
    "plot_waveform(audio)\n",
    "\n",
    "audio = adaptive_plosive_reduction(audio, spike_threshold=22000, reduction_factor=0.5)\n",
    "plot_waveform(audio)\n",
    "audio = boost_audio(audio, factor=1)\n",
    "\n",
    "plot_waveform(audio)\n",
    "play_audio(audio)"
   ],
   "id": "da3cd8fc965490b8",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
