{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# CV5 - Spark - RDDs"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b40d9ec9a1dc8c8f"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-01T09:49:07.846717354Z",
     "start_time": "2023-12-01T09:49:03.685566731Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/12/01 10:49:05 WARN Utils: Your hostname, gentuwu resolves to a loopback address: 127.0.0.1; using 147.230.1.249 instead (on interface wlo1)\n",
      "23/12/01 10:49:05 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/12/01 10:49:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/12/01 10:49:07 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "<pyspark.sql.session.SparkSession object at 0x7f3d48034250>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"Cv5\").getOrCreate()\n",
    "\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# to stop the session\n",
    "spark.stop()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "30f40414467a83e3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## AVG friend Count by age"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "78a40f817618992"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---+-------+\n",
      "|userID|    name|age|friends|\n",
      "+------+--------+---+-------+\n",
      "|     0|    Will| 33|    385|\n",
      "|     1|Jean-Luc| 26|      2|\n",
      "|     2|    Hugh| 55|    221|\n",
      "|     3|  Deanna| 40|    465|\n",
      "|     4|   Quark| 68|     21|\n",
      "|     5|  Weyoun| 59|    318|\n",
      "|     6|  Gowron| 37|    220|\n",
      "|     7|    Will| 54|    307|\n",
      "|     8|  Jadzia| 38|    380|\n",
      "|     9|    Hugh| 27|    181|\n",
      "|    10|     Odo| 53|    191|\n",
      "|    11|     Ben| 57|    372|\n",
      "|    12|   Keiko| 54|    253|\n",
      "|    13|Jean-Luc| 56|    444|\n",
      "|    14|    Hugh| 43|     49|\n",
      "|    15|     Rom| 36|     49|\n",
      "|    16|  Weyoun| 22|    323|\n",
      "|    17|     Odo| 35|     13|\n",
      "|    18|Jean-Luc| 45|    455|\n",
      "|    19|  Geordi| 60|    246|\n",
      "+------+--------+---+-------+\n"
     ]
    }
   ],
   "source": [
    "# read the file located at ./data/fakefriends-header.csv \n",
    "# the file is located locally on the machine not on HDFS\n",
    "\n",
    "# read the file\n",
    "# Specify the path to your CSV file\n",
    "csv_file_path = \"./data/fakefriends-header.csv\"\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Show the first few rows of the DataFrame\n",
    "df.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T09:49:19.357933080Z",
     "start_time": "2023-12-01T09:49:14.300574736Z"
    }
   },
   "id": "1ebf2e594511a484"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "|age|avg_friends|\n",
      "+---+-----------+\n",
      "| 31|     267.25|\n",
      "| 65|      298.2|\n",
      "| 53|     222.86|\n",
      "| 34|      245.5|\n",
      "| 28|      209.1|\n",
      "| 26|     242.06|\n",
      "| 27|     228.13|\n",
      "| 44|     282.17|\n",
      "| 22|     206.43|\n",
      "| 47|     233.22|\n",
      "| 52|     340.64|\n",
      "| 40|     250.82|\n",
      "| 20|      165.0|\n",
      "| 57|     258.83|\n",
      "| 54|     278.08|\n",
      "| 48|      281.4|\n",
      "| 19|     213.27|\n",
      "| 64|     281.33|\n",
      "| 41|     268.56|\n",
      "| 43|     230.57|\n",
      "+---+-----------+\n"
     ]
    }
   ],
   "source": [
    "# learn the average number of friends by age\n",
    "# group by age and compute the average number of friends\n",
    "# round it to 2 decimal places and renames the columns as age and avg_friends\n",
    "\n",
    "from pyspark.sql.functions import round\n",
    "\n",
    "result_df = df.groupBy(\"age\").avg(\"friends\").select(\n",
    "    df[\"age\"].alias(\"age\"),\n",
    "    round(\"avg(friends)\", 2).alias(\"avg_friends\")\n",
    ")\n",
    "\n",
    "result_df.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T09:49:24.880008857Z",
     "start_time": "2023-12-01T09:49:24.131583163Z"
    }
   },
   "id": "36a2db23201c9d18"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## AVG count of orders by age for each customer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "721fb06ef01fa95a"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+\n",
      "|_c0| _c1|  _c2|\n",
      "+---+----+-----+\n",
      "| 44|8602|37.19|\n",
      "| 35|5368|65.89|\n",
      "|  2|3391|40.64|\n",
      "| 47|6694|14.98|\n",
      "| 29| 680|13.08|\n",
      "| 91|8900|24.59|\n",
      "| 70|3959|68.68|\n",
      "| 85|1733|28.53|\n",
      "| 53|9900|83.55|\n",
      "| 14|1505| 4.32|\n",
      "| 51|3378| 19.8|\n",
      "| 42|6926|57.77|\n",
      "|  2|4424|55.77|\n",
      "| 79|9291|33.17|\n",
      "| 50|3901|23.57|\n",
      "| 20|6633| 6.49|\n",
      "| 15|6148|65.53|\n",
      "| 44|8331|99.19|\n",
      "|  5|3505|64.18|\n",
      "| 48|5539|32.42|\n",
      "+---+----+-----+\n"
     ]
    }
   ],
   "source": [
    "# read the file located at ./data/customer-orders.csv\n",
    "# the file is located locally on the machine not on HDFS\n",
    "\n",
    "# read the file\n",
    "# Specify the path to your CSV file\n",
    "csv_file_path = \"./data/customer-orders.csv\"\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = spark.read.csv(csv_file_path, header=False, inferSchema=True)\n",
    "\n",
    "# Show the first few rows of the DataFrame\n",
    "df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T09:49:47.646744964Z",
     "start_time": "2023-12-01T09:49:47.067695372Z"
    }
   },
   "id": "2d28b359257c841d"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|customer|total_spent|\n",
      "+--------+-----------+\n",
      "|      68|    6375.45|\n",
      "|      73|     6206.2|\n",
      "|      39|    6193.11|\n",
      "|      54|    6065.39|\n",
      "|      71|    5995.66|\n",
      "|       2|    5994.59|\n",
      "|      97|    5977.19|\n",
      "|      46|    5963.11|\n",
      "|      42|    5696.84|\n",
      "|      59|    5642.89|\n",
      "|      41|    5637.62|\n",
      "|       0|    5524.95|\n",
      "|       8|    5517.24|\n",
      "|      85|    5503.43|\n",
      "|      61|    5497.48|\n",
      "|      32|    5496.05|\n",
      "|      58|    5437.73|\n",
      "|      63|    5415.15|\n",
      "|      15|    5413.51|\n",
      "|       6|    5397.88|\n",
      "+--------+-----------+\n"
     ]
    }
   ],
   "source": [
    "# learn the total value, 3rd column, spent by each customer\n",
    "# group by customer id, 1st column, and compute the sum of the 3rd column\n",
    "# round it to 2 decimal places and renames the columns as customer and total_spent\n",
    "\n",
    "from pyspark.sql.functions import round\n",
    "\n",
    "# Assuming \"_c0\" is the column you want to rename as \"customer\" and \"_c2\" as \"total_spent\"\n",
    "result_df = df.groupBy(\"_c0\").sum(\"_c2\").select(\n",
    "    df[\"_c0\"].alias(\"customer\"),\n",
    "    round(\"sum(_c2)\", 2).alias(\"total_spent\")\n",
    ").orderBy(\"total_spent\", ascending=False)\n",
    "\n",
    "result_df.show()\n",
    "\n",
    "# CHYBI NAZVY SLOUPCU v .csv souboru"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T09:50:04.836907900Z",
     "start_time": "2023-12-01T09:50:04.369565862Z"
    }
   },
   "id": "e9e3d5078055ae15"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SUPERHEROES\n",
    "\n",
    "- marvel-names.txt neni utf-8"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6597a03059f2e6c1"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|                name|\n",
      "+---+--------------------+\n",
      "|  1|24-HOUR MAN/EMMANUEL|\n",
      "|  2|3-D MAN/CHARLES CHAN|\n",
      "|  3|    4-D MAN/MERCURIO|\n",
      "|  4|             8-BALL/|\n",
      "|  5|                   A|\n",
      "|  6|               A'YIN|\n",
      "|  7|        ABBOTT, JACK|\n",
      "|  8|             ABCISSA|\n",
      "|  9|                ABEL|\n",
      "| 10|ABOMINATION/EMIL BLO|\n",
      "| 11|ABOMINATION | MUTANT|\n",
      "| 12|         ABOMINATRIX|\n",
      "| 13|             ABRAXAS|\n",
      "| 14|          ADAM 3,031|\n",
      "| 15|             ABSALOM|\n",
      "| 16|ABSORBING MAN/CARL C|\n",
      "| 17|ABSORBING MAN | MUTA|\n",
      "| 18|                ACBA|\n",
      "| 19|ACHEBE, REVEREND DOC|\n",
      "| 20|            ACHILLES|\n",
      "+---+--------------------+\n"
     ]
    }
   ],
   "source": [
    "# load the file located at ./data/marvel-names.txt\n",
    "# the file is located locally on the machine not on HDFS\n",
    "\n",
    "from pyspark.sql.functions import col, split\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Specify the path to your text file\n",
    "txt_file_path = \"./data/marvel-names.txt\"\n",
    "\n",
    "schema = StructType([ \\\n",
    "    StructField(\"id\", IntegerType(), True), \\\n",
    "    StructField(\"name\", StringType(), True)])\n",
    "\n",
    "names = spark.read.schema(schema).option(\"sep\", \" \").csv(txt_file_path)\n",
    "\n",
    "names.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T09:50:19.910743618Z",
     "start_time": "2023-12-01T09:50:19.743413619Z"
    }
   },
   "id": "835518dfc0cd5e7f"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+\n",
      "|  id|connections|\n",
      "+----+-----------+\n",
      "| 691|          6|\n",
      "|1159|         11|\n",
      "|3959|        142|\n",
      "|1572|         35|\n",
      "|2294|         14|\n",
      "|1090|          4|\n",
      "|3606|        171|\n",
      "|3414|          7|\n",
      "| 296|         17|\n",
      "|4821|         16|\n",
      "|2162|         41|\n",
      "|1436|          9|\n",
      "|1512|         11|\n",
      "|6194|         14|\n",
      "|6240|         11|\n",
      "| 829|         37|\n",
      "|2136|          6|\n",
      "|5645|         20|\n",
      "|2069|        263|\n",
      "| 467|          0|\n",
      "+----+-----------+\n"
     ]
    }
   ],
   "source": [
    "# load the file located at ./data/marvel-graph.txt\n",
    "# the file is located locally on the machine not on HDFS\n",
    "# the first number in the line represents the superhero id and the rest of the numbers in the line are the superhero ids of the superheroes the first superhero has appeared with\n",
    "\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql import functions as func\n",
    "\n",
    "# read the file\n",
    "# Specify the path to your text file\n",
    "txt_file_path = \"./data/marvel-graph.txt\"\n",
    "\n",
    "lines = spark.read.text(txt_file_path)\n",
    "\n",
    "connections = lines.withColumn(\"id\", func.split(func.trim(func.col(\"value\")), \" \")[0]) \\\n",
    "    .withColumn(\"connections\", func.size(func.split(func.trim(func.col(\"value\")), \" \")) - 1) \\\n",
    "    .groupBy(\"id\").agg(func.sum(\"connections\").alias(\"connections\"))\n",
    "\n",
    "connections.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T09:50:29.351476249Z",
     "start_time": "2023-12-01T09:50:28.872193051Z"
    }
   },
   "id": "c27f47fb7c93a258"
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAPTAIN AMERICA is the most popular superhero with 1933 co-appearances.\n"
     ]
    }
   ],
   "source": [
    "mostPopular = connections.sort(func.col(\"connections\").desc()).first()\n",
    "\n",
    "# print the most popular superhero his name and the number of connections\\\n",
    "\n",
    "mostPopularName = names.filter(func.col(\"id\") == mostPopular[0]).select(\"name\").first()\n",
    "\n",
    "print(mostPopularName[0] + \" is the most popular superhero with \" + str(mostPopular[1]) + \" co-appearances.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T21:03:06.627926864Z",
     "start_time": "2023-11-24T21:03:06.335440536Z"
    }
   },
   "id": "94885faf5ba40647"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|                name|connections|\n",
      "+--------------------+-----------+\n",
      "|                GOOM|          1|\n",
      "|HOUSTON, LT. COMMAND|          1|\n",
      "|         MARAUDER II|          1|\n",
      "| MASTER OF VENGEANCE|          1|\n",
      "|               MOTH/|          1|\n",
      "|              NIMROD|          1|\n",
      "|   PROFESSOR GIBBON/|          1|\n",
      "|                SAJA|          1|\n",
      "|             SLAYER/|          1|\n",
      "|SPARROW BEAR, MELLIS|          1|\n",
      "+--------------------+-----------+\n"
     ]
    }
   ],
   "source": [
    "# find the top 10 superheroes with the least co-appearances \n",
    "# print their names and the number of co-appearances\n",
    "# sort the result by the number of co-appearances in ascending order\n",
    "# the coo-appearances cannot be 0\n",
    "\n",
    "leastPopular = connections.sort(func.col(\"connections\").asc()).filter(func.col(\"connections\") > 0).limit(10)\n",
    "\n",
    "leastPopularNames = names.join(leastPopular, \"id\").select(\"name\", \"connections\")\n",
    "\n",
    "leastPopularNames.show()\n",
    "\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T09:50:40.158784439Z",
     "start_time": "2023-12-01T09:50:39.406396947Z"
    }
   },
   "id": "d2c34ea77515f114"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f194ac3bea4917f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
