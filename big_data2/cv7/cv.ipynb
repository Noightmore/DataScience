{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# CV 7\n",
    "\n",
    " to install netcat:\n",
    "    apt update\n",
    "    apt-get install netcat\n",
    "    nc -lkp 9999 &"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7a0b6e78e937423c"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, split, lower, col,regexp_replace\n",
    "\n",
    "spark = SparkSession.builder.appName(\"NetworkWordCount\").getOrCreate()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "spark.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T11:01:41.419216337Z",
     "start_time": "2023-12-01T11:01:41.049665728Z"
    }
   },
   "id": "aa8f0b23ead78357"
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/12/01 12:01:43 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-e07249ac-ef24-466b-a321-44cfa6a8c326. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/12/01 12:01:43 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "+-----+----+-----+\n",
      "|start|word|count|\n",
      "+-----+----+-----+\n",
      "+-----+----+-----+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+------------+-----+\n",
      "|start                                     |word        |count|\n",
      "+------------------------------------------+------------+-----+\n",
      "|{2023-12-01 12:01:30, 2023-12-01 12:02:00}|bruh        |21   |\n",
      "|{2023-12-01 12:01:30, 2023-12-01 12:02:00}|here        |5    |\n",
      "|{2023-12-01 12:01:30, 2023-12-01 12:02:00}|hello       |2    |\n",
      "|{2023-12-01 12:01:30, 2023-12-01 12:02:00}|bruhbruhbruh|1    |\n",
      "|{2023-12-01 12:01:30, 2023-12-01 12:02:00}|dfaasf      |1    |\n",
      "|{2023-12-01 12:01:30, 2023-12-01 12:02:00}|aaaaaaaaaa  |1    |\n",
      "|{2023-12-01 12:01:30, 2023-12-01 12:02:00}|dsdsdasf    |1    |\n",
      "|{2023-12-01 12:01:30, 2023-12-01 12:02:00}|count       |1    |\n",
      "|{2023-12-01 12:01:30, 2023-12-01 12:02:00}|to          |1    |\n",
      "|{2023-12-01 12:01:30, 2023-12-01 12:02:00}|now         |1    |\n",
      "|{2023-12-01 12:01:30, 2023-12-01 12:02:00}|world       |1    |\n",
      "|{2023-12-01 12:01:30, 2023-12-01 12:02:00}|hele        |1    |\n",
      "|{2023-12-01 12:01:30, 2023-12-01 12:02:00}|on          |1    |\n",
      "|{2023-12-01 12:01:30, 2023-12-01 12:02:00}|co          |1    |\n",
      "+------------------------------------------+------------+-----+\n",
      "+------------------------------------------+------------+-----+\n",
      "|start                                     |word        |count|\n",
      "+------------------------------------------+------------+-----+\n",
      "|{2023-12-01 12:01:30, 2023-12-01 12:02:00}|bruh        |21   |\n",
      "|{2023-12-01 12:01:30, 2023-12-01 12:02:00}|here        |5    |\n",
      "|{2023-12-01 12:01:30, 2023-12-01 12:02:00}|hello       |2    |\n",
      "|{2023-12-01 12:01:30, 2023-12-01 12:02:00}|bruhbruhbruh|1    |\n",
      "|{2023-12-01 12:01:30, 2023-12-01 12:02:00}|dfaasf      |1    |\n",
      "|{2023-12-01 12:01:30, 2023-12-01 12:02:00}|aaaaaaaaaa  |1    |\n",
      "|{2023-12-01 12:01:30, 2023-12-01 12:02:00}|dsdsdasf    |1    |\n",
      "|{2023-12-01 12:01:30, 2023-12-01 12:02:00}|count       |1    |\n",
      "|{2023-12-01 12:01:30, 2023-12-01 12:02:00}|to          |1    |\n",
      "|{2023-12-01 12:01:30, 2023-12-01 12:02:00}|now         |1    |\n",
      "|{2023-12-01 12:01:30, 2023-12-01 12:02:00}|world       |1    |\n",
      "|{2023-12-01 12:01:30, 2023-12-01 12:02:00}|hele        |1    |\n",
      "|{2023-12-01 12:01:30, 2023-12-01 12:02:00}|on          |1    |\n",
      "|{2023-12-01 12:01:30, 2023-12-01 12:02:00}|co          |1    |\n",
      "+------------------------------------------+------------+-----+\n",
      "23/12/01 12:01:55 ERROR MicroBatchExecution: Query my_query [id = 7e860307-f1af-4230-b3ef-5c5453bcd420, runId = 6f04a8d6-14d3-4a76-ab2b-7d23d01b2521] terminated with error\n",
      "java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\n",
      "This stopped SparkContext was created at:\n",
      "\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\n",
      "java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)\n",
      "java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.base/java.lang.Thread.run(Thread.java:833)\n",
      "\n",
      "The currently active SparkContext was created at:\n",
      "\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\n",
      "java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)\n",
      "java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.base/java.lang.Thread.run(Thread.java:833)\n",
      "         \n",
      "\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:120)\n",
      "\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1526)\n",
      "\tat org.apache.spark.sql.execution.datasources.text.TextFileFormat.buildReader(TextFileFormat.scala:106)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues(FileFormat.scala:132)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues$(FileFormat.scala:123)\n",
      "\tat org.apache.spark.sql.execution.datasources.TextBasedFileFormat.buildReaderWithPartitionValues(FileFormat.scala:232)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:448)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec.doExecute(DataSourceScanExec.scala:535)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:194)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:232)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:229)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:190)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:527)\n",
      "\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:455)\n",
      "\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:454)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:498)\n",
      "\tat org.apache.spark.sql.execution.GenerateExec.inputRDDs(GenerateExec.scala:130)\n",
      "\tat org.apache.spark.sql.execution.FilterExec.inputRDDs(basicPhysicalOperators.scala:238)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:51)\n",
      "\tat org.apache.spark.sql.execution.ExpandExec.inputRDDs(ExpandExec.scala:90)\n",
      "\tat org.apache.spark.sql.execution.aggregate.AggregateCodegenSupport.inputRDDs(AggregateCodegenSupport.scala:89)\n",
      "\tat org.apache.spark.sql.execution.aggregate.AggregateCodegenSupport.inputRDDs$(AggregateCodegenSupport.scala:88)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:47)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:751)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:194)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:232)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:229)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:190)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD$lzycompute(ShuffleExchangeExec.scala:135)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD(ShuffleExchangeExec.scala:135)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:169)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:167)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:189)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:194)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:232)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:229)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:190)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:527)\n",
      "\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:455)\n",
      "\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:454)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:498)\n",
      "\tat org.apache.spark.sql.execution.aggregate.AggregateCodegenSupport.inputRDDs(AggregateCodegenSupport.scala:89)\n",
      "\tat org.apache.spark.sql.execution.aggregate.AggregateCodegenSupport.inputRDDs$(AggregateCodegenSupport.scala:88)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:47)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:751)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:194)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:232)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:229)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:190)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreRestoreExec.doExecute(statefulOperators.scala:306)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:194)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:232)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:229)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:190)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:527)\n",
      "\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:455)\n",
      "\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:454)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:498)\n",
      "\tat org.apache.spark.sql.execution.aggregate.AggregateCodegenSupport.inputRDDs(AggregateCodegenSupport.scala:89)\n",
      "\tat org.apache.spark.sql.execution.aggregate.AggregateCodegenSupport.inputRDDs$(AggregateCodegenSupport.scala:88)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:47)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:751)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:194)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:232)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:229)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:190)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.doExecute(statefulOperators.scala:369)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:194)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:232)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:229)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:190)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:527)\n",
      "\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:455)\n",
      "\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:454)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:498)\n",
      "\tat org.apache.spark.sql.execution.aggregate.AggregateCodegenSupport.inputRDDs(AggregateCodegenSupport.scala:89)\n",
      "\tat org.apache.spark.sql.execution.aggregate.AggregateCodegenSupport.inputRDDs$(AggregateCodegenSupport.scala:88)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:47)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:751)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:194)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:232)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:229)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:190)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD$lzycompute(ShuffleExchangeExec.scala:135)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD(ShuffleExchangeExec.scala:135)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:169)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:167)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:189)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:194)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:232)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:229)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:190)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:527)\n",
      "\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:455)\n",
      "\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:454)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:498)\n",
      "\tat org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:132)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:751)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:194)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:232)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:229)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:190)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:355)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:353)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:302)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:313)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n",
      "\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3868)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3120)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\n",
      "\tat org.apache.spark.sql.Dataset.collect(Dataset.scala:3120)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:663)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:658)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:658)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:255)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:218)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:212)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)\n",
      "23/12/01 12:01:55 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@556bb4c3[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@20f53e1c[Wrapped task = org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1@7314611e]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@5de63656[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread \"stream execution thread for my_query [id = 7e860307-f1af-4230-b3ef-5c5453bcd420, runId = 6f04a8d6-14d3-4a76-ab2b-7d23d01b2521]\" org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef.deactivateInstances(StateStoreCoordinator.scala:119)\n",
      "\tat org.apache.spark.sql.streaming.StreamingQueryManager.notifyQueryTermination(StreamingQueryManager.scala:406)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$3(StreamExecution.scala:357)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:338)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)\n",
      "Caused by: org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:176)\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:144)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.askAbortable(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.askAbortable(NettyRpcEnv.scala:554)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:558)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:102)\n",
      "\t... 8 more\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+------------+-----+\n",
      "|start                                     |word        |count|\n",
      "+------------------------------------------+------------+-----+\n",
      "|{2023-12-01 12:01:30, 2023-12-01 12:02:00}|bruh        |21   |\n",
      "|{2023-12-01 12:01:30, 2023-12-01 12:02:00}|here        |5    |\n",
      "|{2023-12-01 12:01:30, 2023-12-01 12:02:00}|hello       |2    |\n",
      "|{2023-12-01 12:01:30, 2023-12-01 12:02:00}|bruhbruhbruh|1    |\n",
      "|{2023-12-01 12:01:30, 2023-12-01 12:02:00}|dfaasf      |1    |\n",
      "|{2023-12-01 12:01:30, 2023-12-01 12:02:00}|aaaaaaaaaa  |1    |\n",
      "|{2023-12-01 12:01:30, 2023-12-01 12:02:00}|dsdsdasf    |1    |\n",
      "|{2023-12-01 12:01:30, 2023-12-01 12:02:00}|count       |1    |\n",
      "|{2023-12-01 12:01:30, 2023-12-01 12:02:00}|to          |1    |\n",
      "|{2023-12-01 12:01:30, 2023-12-01 12:02:00}|now         |1    |\n",
      "|{2023-12-01 12:01:30, 2023-12-01 12:02:00}|world       |1    |\n",
      "|{2023-12-01 12:01:30, 2023-12-01 12:02:00}|hele        |1    |\n",
      "|{2023-12-01 12:01:30, 2023-12-01 12:02:00}|on          |1    |\n",
      "|{2023-12-01 12:01:30, 2023-12-01 12:02:00}|co          |1    |\n",
      "+------------------------------------------+------------+-----+\n",
      "+------------------------------------------+----+-----+\n",
      "|start                                     |word|count|\n",
      "+------------------------------------------+----+-----+\n",
      "|{2023-12-01 12:01:45, 2023-12-01 12:02:15}|bruh|3    |\n",
      "+------------------------------------------+----+-----+\n",
      "+------------------------------------------+----+-----+\n",
      "|start                                     |word|count|\n",
      "+------------------------------------------+----+-----+\n",
      "|{2023-12-01 12:01:45, 2023-12-01 12:02:15}|bruh|3    |\n",
      "+------------------------------------------+----+-----+\n",
      "+------------------------------------------+----+-----+\n",
      "|start                                     |word|count|\n",
      "+------------------------------------------+----+-----+\n",
      "|{2023-12-01 12:01:45, 2023-12-01 12:02:15}|bruh|3    |\n",
      "+------------------------------------------+----+-----+\n",
      "+------------------------------------------+----+-----+\n",
      "|start                                     |word|count|\n",
      "+------------------------------------------+----+-----+\n",
      "|{2023-12-01 12:01:45, 2023-12-01 12:02:15}|bruh|3    |\n",
      "+------------------------------------------+----+-----+\n",
      "+------------------------------------------+----+-----+\n",
      "|start                                     |word|count|\n",
      "+------------------------------------------+----+-----+\n",
      "|{2023-12-01 12:01:45, 2023-12-01 12:02:15}|bruh|3    |\n",
      "+------------------------------------------+----+-----+\n",
      "+------------------------------------------+----+-----+\n",
      "|start                                     |word|count|\n",
      "+------------------------------------------+----+-----+\n",
      "|{2023-12-01 12:01:45, 2023-12-01 12:02:15}|bruh|3    |\n",
      "+------------------------------------------+----+-----+\n",
      "+------------------------------------------+----+-----+\n",
      "|start                                     |word|count|\n",
      "+------------------------------------------+----+-----+\n",
      "|{2023-12-01 12:01:45, 2023-12-01 12:02:15}|bruh|3    |\n",
      "+------------------------------------------+----+-----+\n",
      "+------------------------------------------+----+-----+\n",
      "|start                                     |word|count|\n",
      "+------------------------------------------+----+-----+\n",
      "|{2023-12-01 12:01:45, 2023-12-01 12:02:15}|bruh|3    |\n",
      "+------------------------------------------+----+-----+\n",
      "+------------------------------------------+----+-----+\n",
      "|start                                     |word|count|\n",
      "+------------------------------------------+----+-----+\n",
      "|{2023-12-01 12:01:45, 2023-12-01 12:02:15}|bruh|3    |\n",
      "+------------------------------------------+----+-----+\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[77], line 33\u001B[0m\n\u001B[1;32m     25\u001B[0m latest_window_query \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[1;32m     26\u001B[0m \u001B[38;5;124mSELECT *\u001B[39m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;124mFROM my_query\u001B[39m\n\u001B[1;32m     28\u001B[0m \u001B[38;5;124mWHERE start = (SELECT MAX(start) FROM my_query)\u001B[39m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;124m\u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[1;32m     31\u001B[0m spark\u001B[38;5;241m.\u001B[39msql(latest_window_query)\u001B[38;5;241m.\u001B[39mshow(truncate\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m---> 33\u001B[0m time\u001B[38;5;241m.\u001B[39msleep(\u001B[38;5;241m5\u001B[39m)\n",
      "File \u001B[0;32m~/Programming/DataScience/venv/lib/python3.11/site-packages/pyspark/context.py:363\u001B[0m, in \u001B[0;36mSparkContext._do_init.<locals>.signal_handler\u001B[0;34m(signal, frame)\u001B[0m\n\u001B[1;32m    361\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msignal_handler\u001B[39m(signal: Any, frame: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m NoReturn:\n\u001B[1;32m    362\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcancelAllJobs()\n\u001B[0;32m--> 363\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m()\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, split, lower, col,regexp_replace,current_timestamp, window, desc\n",
    "\n",
    "spark = SparkSession.builder.appName(\"NetworkWordCount\").getOrCreate()\n",
    "lines = spark.readStream.option('newFilesOnly', 'true').text('./data')\n",
    "#lines = spark.readStream.format(\"socket\").option(\"host\", \"localhost\").option(\"port\", 9999).load()\n",
    "\n",
    "words = lines.select(explode(split(lines.value, \" \")).alias(\"word\"))\n",
    "\n",
    "words = words.select(lower(col('word')).alias('word'))\n",
    "words = words.withColumn('word', regexp_replace('word', r'[^\\s\\w]', ''))\n",
    "words = words.filter(\"word != ''\")\n",
    "\n",
    "words = words.withColumn('eventTime', current_timestamp())\n",
    "wordCounts = words.groupBy(window(col(\"eventTime\"), \"30 seconds\", \"15 seconds\").alias('start'), col(\"word\")).count()\n",
    "wordSorted = wordCounts.orderBy(\"start\",desc(\"count\"))\n",
    "\n",
    "\n",
    "query = wordSorted.writeStream.outputMode(\"complete\").format(\"memory\").queryName(\"my_query\").start()\n",
    "\n",
    "while True:\n",
    "    #spark.sql(\"SELECT * FROM my_query\").show(truncate=False)\n",
    "    # just show 1 window each time\n",
    "    latest_window_query = \"\"\"\n",
    "    SELECT *\n",
    "    FROM my_query\n",
    "    WHERE start = (SELECT MAX(start) FROM my_query)\n",
    "    \"\"\"\n",
    "\n",
    "    spark.sql(latest_window_query).show(truncate=False)\n",
    "\n",
    "    time.sleep(5)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T11:02:47.414175920Z",
     "start_time": "2023-12-01T11:01:43.410997994Z"
    }
   },
   "id": "9b69891da54f2c94"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# cv 7 bonus"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9eb1a9ff573a76b1"
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 35:===========================================>          (160 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/12/01 11:59:19 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-40e17a3b-c4e8-4155-8077-d25fd922dfd4. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/12/01 11:59:19 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 35:=============================================>        (168 + 8) / 200]\r"
     ]
    },
    {
     "ename": "IllegalArgumentException",
     "evalue": "Cannot start query with name my_query as a query with that name is already active in this SparkSession",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIllegalArgumentException\u001B[0m                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[72], line 25\u001B[0m\n\u001B[1;32m     21\u001B[0m wordCounts \u001B[38;5;241m=\u001B[39m words\u001B[38;5;241m.\u001B[39mgroupBy(window(col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124meventTime\u001B[39m\u001B[38;5;124m\"\u001B[39m), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m30 seconds\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m15 seconds\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstart\u001B[39m\u001B[38;5;124m'\u001B[39m), col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mword\u001B[39m\u001B[38;5;124m\"\u001B[39m))\u001B[38;5;241m.\u001B[39mcount()\n\u001B[1;32m     23\u001B[0m wordSorted \u001B[38;5;241m=\u001B[39m wordCounts\u001B[38;5;241m.\u001B[39morderBy(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstart\u001B[39m\u001B[38;5;124m\"\u001B[39m,desc(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcount\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[0;32m---> 25\u001B[0m query \u001B[38;5;241m=\u001B[39m \u001B[43mwordSorted\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwriteStream\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moutputMode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcomplete\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformat\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmemory\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mqueryName\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmy_query\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstart\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m     28\u001B[0m     \u001B[38;5;66;03m#spark.sql(\"SELECT * FROM my_query\").show(truncate=False)\u001B[39;00m\n\u001B[1;32m     29\u001B[0m     \u001B[38;5;66;03m# just show 1 window each time\u001B[39;00m\n\u001B[1;32m     30\u001B[0m     latest_window_query \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;124m    SELECT *\u001B[39m\n\u001B[1;32m     32\u001B[0m \u001B[38;5;124m    FROM my_query\u001B[39m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;124m    WHERE start = (SELECT MAX(start) FROM my_query)\u001B[39m\n\u001B[1;32m     34\u001B[0m \u001B[38;5;124m    \u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m\n",
      "File \u001B[0;32m~/Programming/DataScience/venv/lib/python3.11/site-packages/pyspark/sql/streaming.py:1389\u001B[0m, in \u001B[0;36mDataStreamWriter.start\u001B[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001B[0m\n\u001B[1;32m   1387\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mqueryName(queryName)\n\u001B[1;32m   1388\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m path \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1389\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sq(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstart\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m   1390\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1391\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sq(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jwrite\u001B[38;5;241m.\u001B[39mstart(path))\n",
      "File \u001B[0;32m~/Programming/DataScience/venv/lib/python3.11/site-packages/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
      "File \u001B[0;32m~/Programming/DataScience/venv/lib/python3.11/site-packages/pyspark/sql/utils.py:196\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    192\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    193\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    194\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    195\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 196\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    197\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    198\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[0;31mIllegalArgumentException\u001B[0m: Cannot start query with name my_query as a query with that name is already active in this SparkSession"
     ]
    }
   ],
   "source": [
    "# the same thing as above but this time when a new file is added to the directory, the whole directory is read again including the new file\n",
    "\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, split, lower, col,regexp_replace,current_timestamp, window, desc\n",
    "\n",
    "spark = SparkSession.builder.appName(\"NetworkWordCount\").getOrCreate()\n",
    "\n",
    "lines = spark.readStream.option('newFilesOnly', 'true').text('./data')\n",
    "\n",
    "words = lines.select(explode(split(lines.value, \" \")).alias(\"word\"))\n",
    "\n",
    "words = words.select(lower(col('word')).alias('word'))\n",
    "\n",
    "words = words.withColumn('word', regexp_replace('word', r'[^\\s\\w]', ''))\n",
    "\n",
    "words = words.filter(\"word != ''\")\n",
    "\n",
    "words = words.withColumn('eventTime', current_timestamp())\n",
    "\n",
    "wordCounts = words.groupBy(window(col(\"eventTime\"), \"30 seconds\", \"15 seconds\").alias('start'), col(\"word\")).count()\n",
    "\n",
    "wordSorted = wordCounts.orderBy(\"start\",desc(\"count\"))\n",
    "\n",
    "query = wordSorted.writeStream.outputMode(\"complete\").format(\"memory\").queryName(\"my_query\").start()\n",
    "\n",
    "while True:\n",
    "    #spark.sql(\"SELECT * FROM my_query\").show(truncate=False)\n",
    "    # just show 1 window each time\n",
    "    latest_window_query = \"\"\"\n",
    "    SELECT *\n",
    "    FROM my_query\n",
    "    WHERE start = (SELECT MAX(start) FROM my_query)\n",
    "    \"\"\"\n",
    "\n",
    "    spark.sql(latest_window_query).show(truncate=False)\n",
    "\n",
    "    time.sleep(5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T10:59:19.476600795Z",
     "start_time": "2023-12-01T10:59:18.885290796Z"
    }
   },
   "id": "e8148640de7fab2d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
